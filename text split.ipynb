{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,re,collections,nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stw = stopwords.words('MyEnglish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则表达式过滤特殊符号用空格符占位，双引号、单引号、句点、逗号\n",
    "pat_letter = re.compile(r'[^a-zA-Z \\']+')\n",
    "# 还原常见缩写单词\n",
    "pat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n",
    "pat_s = re.compile(\"(?<=[a-zA-Z])\\'s\") # 找出字母后面的字母\n",
    "pat_s2 = re.compile(\"(?<=s)\\'s?\")      \n",
    "pat_not = re.compile(\"(?<=[a-zA-Z])n\\'t\") # not的缩写\n",
    "pat_would = re.compile(\"(?<=[a-zA-Z])\\'d\") # would的缩写\n",
    "pat_will = re.compile(\"(?<=[a-zA-Z])\\'ll\") # will的缩写\n",
    "pat_am = re.compile(\"(?<=[I|i])\\'m\") # am的缩写\n",
    "pat_are = re.compile(\"(?<=[a-zA-Z])\\'re\") # are的缩写\n",
    "pat_ve = re.compile(\"(?<=[a-zA-Z])\\'ve\") # have的缩写\n",
    "\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviations(text):\n",
    "    new_text = text\n",
    "    new_text = pat_letter.sub(' ', text).strip().lower()\n",
    "    new_text = pat_is.sub(r\"\\1 is\", new_text)\n",
    "    new_text = pat_s.sub(\"\", new_text)\n",
    "    new_text = pat_s2.sub(\"\", new_text)\n",
    "    new_text = pat_not.sub(\" not\", new_text)\n",
    "    new_text = pat_would.sub(\" would\", new_text)\n",
    "    new_text = pat_will.sub(\" will\", new_text)\n",
    "    new_text = pat_am.sub(\" am\", new_text)\n",
    "    new_text = pat_are.sub(\" are\", new_text)\n",
    "    new_text = pat_ve.sub(\" have\", new_text)\n",
    "    new_text = new_text.replace('\\'', ' ')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos和tag有相似的地方，通过tag获得pos\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def merge(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stw and wordnet.synsets(word):\n",
    "            tag = nltk.pos_tag(word_tokenize(word)) # tag is like [('bigger', 'JJR')]\n",
    "            pos = get_wordnet_pos(tag[0][1])\n",
    "            if pos:\n",
    "                # lemmatize()方法将word单词还原成pos词性的形式\n",
    "                lemmatized_word = lmtzr.lemmatize(word, pos)\n",
    "                if lemmatized_word not in stw and wordnet.synsets(lemmatized_word):\n",
    "                    new_words.append(lemmatized_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file):  \n",
    "    with open (file) as f:  \n",
    "        words_box=[]\n",
    "        # pat = re.compile(r'[^a-zA-Z \\']+') # 过滤特殊符号\n",
    "        for line in f:                           \n",
    "            words_box.extend(merge(replace_abbreviations(line).split()))\n",
    "    return collections.Counter(words_box) # 返回单词和词频\n",
    "\n",
    "\n",
    "# 将统计结果写入文件\n",
    "def write_to_file(words, file=\"english2.csv\"):\n",
    "    f = open(file, 'w')\n",
    "    for item in words:\n",
    "        for field in item:\n",
    "            f.write(str(field)+',')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting...\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print (\"counting...\")\n",
    "    words = get_words(\"english2.txt\")\n",
    "    write_to_file((words.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[res['次数']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[res['次数']==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[res['次数']==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[res['次数']>=4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4491"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
